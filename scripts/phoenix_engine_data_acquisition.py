#!/usr/bin/env python3
"""
ì‘ì „ëª…: 'ë¶ˆì‚¬ì¡° ì—”ì§„' ê³ ë„í™” v2 - ë°ì´í„° í™•ë³´ ìŠ¤í¬ë¦½íŠ¸
ëª©í‘œ: ëŒ€ê·œëª¨ ê³µê°œ RPPG ë°ì´í„°ì…‹ì„ í™•ë³´í•˜ì—¬ 95% ì´ìƒ ì •í™•ë„ ë‹¬ì„±
"""

import os
import json
import logging
import requests
import zipfile
import tarfile
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import subprocess
import hashlib
from tqdm import tqdm

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PhoenixEngineDataAcquisition:
    """ë¶ˆì‚¬ì¡° ì—”ì§„ ë°ì´í„° í™•ë³´ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.data_dir = Path("backend/data")
        self.data_dir.mkdir(exist_ok=True)
        
        # ê³µê°œ ë°ì´í„°ì…‹ ì •ë³´
        self.datasets = {
            "VIPL-HR": {
                "description": "2,378ê°œ ë¹„ë””ì˜¤, ë‹¤ì–‘í•œ í™˜ê²½ í¬í•¨",
                "url": "https://github.com/VIPL-ICT/VIPL-HR",
                "size_gb": 15.2,
                "samples": 2378,
                "type": "github_repo"
            },
            "MAHNOB-HCI": {
                "description": "527ê°œ ë¹„ë””ì˜¤, ë‹¤ì¤‘ ì„¼ì„œ ë°ì´í„° í¬í•¨",
                "url": "https://mahnob-db.eu/",
                "size_gb": 8.7,
                "samples": 527,
                "type": "official_website"
            },
            "UBFC-rPPG": {
                "description": "42ê°œ ë¹„ë””ì˜¤, ê¸°ë³¸ ë²¤ì¹˜ë§ˆí¬ìš©",
                "url": "https://github.com/ubicenter/ubfc-rppg",
                "size_gb": 2.1,
                "samples": 42,
                "type": "github_repo"
            }
        }
        
        # ë‹¤ìš´ë¡œë“œ ìƒíƒœ
        self.download_status = {}
        
        logger.info("ğŸš€ ë¶ˆì‚¬ì¡° ì—”ì§„ ë°ì´í„° í™•ë³´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def create_data_structure(self):
        """ë°ì´í„° ì €ì¥ êµ¬ì¡° ìƒì„±"""
        logger.info("ğŸ“ ë°ì´í„° ì €ì¥ êµ¬ì¡° ìƒì„± ì¤‘...")
        
        # ë©”ì¸ ë°ì´í„° ë””ë ‰í† ë¦¬
        directories = [
            "rppg",
            "rppg/vipl_hr",
            "rppg/mahnob_hci", 
            "rppg/ubfc_rppg",
            "voice",
            "metadata",
            "processed",
            "training",
            "validation",
            "testing"
        ]
        
        for dir_path in directories:
            (self.data_dir / dir_path).mkdir(parents=True, exist_ok=True)
            logger.info(f"âœ… ë””ë ‰í† ë¦¬ ìƒì„±: {dir_path}")
    
    def download_vipl_hr_dataset(self):
        """VIPL-HR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""
        logger.info("ğŸ”¥ VIPL-HR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘ (2,378ê°œ ë¹„ë””ì˜¤)")
        
        try:
            # VIPL-HR GitHub ì €ì¥ì†Œ í´ë¡ 
            vipl_dir = self.data_dir / "rppg" / "vip_hr"
            if not vipl_dir.exists():
                logger.info("ğŸ“¥ VIPL-HR GitHub ì €ì¥ì†Œ í´ë¡  ì¤‘...")
                subprocess.run([
                    "git", "clone", 
                    "https://github.com/VIPL-ICT/VIPL-HR.git",
                    str(vipl_dir)
                ], check=True)
                logger.info("âœ… VIPL-HR ì €ì¥ì†Œ í´ë¡  ì™„ë£Œ")
            else:
                logger.info("âœ… VIPL-HR ì €ì¥ì†Œ ì´ë¯¸ ì¡´ì¬")
            
            # ë°ì´í„° êµ¬ì¡° í™•ì¸
            self._analyze_vip_hr_structure(vipl_dir)
            
        except Exception as e:
            logger.error(f"âŒ VIPL-HR ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
        
        return True
    
    def download_mahnob_hci_dataset(self):
        """MAHNOB-HCI ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""
        logger.info("ğŸ¯ MAHNOB-HCI ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘ (527ê°œ ë¹„ë””ì˜¤)")
        
        try:
            mahnob_dir = self.data_dir / "rppg" / "mahnob_hci"
            mahnob_dir.mkdir(exist_ok=True)
            
            # MAHNOB-HCIëŠ” ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‹ ì²­ í•„ìš”
            # ìë™ ë‹¤ìš´ë¡œë“œ ëŒ€ì‹  ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ ì œê³µ
            guide_file = mahnob_dir / "DOWNLOAD_GUIDE.md"
            
            guide_content = """# MAHNOB-HCI ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ

## ğŸ“‹ ë°ì´í„°ì…‹ ì •ë³´
- **ì´ ë¹„ë””ì˜¤ ìˆ˜**: 527ê°œ
- **í¬ê¸°**: ì•½ 8.7GB
- **íŠ¹ì§•**: ë‹¤ì¤‘ ì„¼ì„œ ë°ì´í„° í¬í•¨

## ğŸ”— ë‹¤ìš´ë¡œë“œ ë§í¬
ê³µì‹ ì›¹ì‚¬ì´íŠ¸: https://mahnob-db.eu/

## ğŸ“ ë‹¤ìš´ë¡œë“œ ì ˆì°¨
1. ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸
2. ê³„ì • ìƒì„± ë° ë¡œê·¸ì¸
3. ë°ì´í„° ì‚¬ìš© ëª©ì  ì‹ ì²­ì„œ ì‘ì„±
4. ìŠ¹ì¸ í›„ ë‹¤ìš´ë¡œë“œ ë§í¬ ì œê³µ
5. ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ì´ ë””ë ‰í† ë¦¬ì— ì••ì¶• í•´ì œ

## ğŸ“ ì••ì¶• í•´ì œ ëª…ë ¹ì–´
```bash
# tar.gz íŒŒì¼ì¸ ê²½ìš°
tar -xzf mahnob_hci_dataset.tar.gz

# zip íŒŒì¼ì¸ ê²½ìš°
unzip mahnob_hci_dataset.zip
```

## âš ï¸ ì£¼ì˜ì‚¬í•­
- ë°ì´í„° ì‚¬ìš© ëª©ì ì„ ëª…í™•íˆ ì‘ì„±í•´ì•¼ ìŠ¹ì¸ë©ë‹ˆë‹¤
- ëŒ€ìš©ëŸ‰ íŒŒì¼ì´ë¯€ë¡œ ì•ˆì •ì ì¸ ë„¤íŠ¸ì›Œí¬ í™˜ê²½ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”
"""
            
            with open(guide_file, 'w', encoding='utf-8') as f:
                f.write(guide_content)
            
            logger.info("ğŸ“‹ MAHNOB-HCI ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ ìƒì„± ì™„ë£Œ")
            logger.info("âš ï¸ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤: https://mahnob-db.eu/")
            
        except Exception as e:
            logger.error(f"âŒ MAHNOB-HCI ê°€ì´ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
        
        return True
    
    def download_ubfc_rppg_dataset(self):
        """UBFC-rPPG ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""
        logger.info("âš¡ UBFC-rPPG ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘ (42ê°œ ë¹„ë””ì˜¤)")
        
        try:
            # UBFC-rPPG GitHub ì €ì¥ì†Œ í´ë¡ 
            ubfc_dir = self.data_dir / "rppg" / "ubfc_rppg"
            if not ubfc_dir.exists():
                logger.info("ğŸ“¥ UBFC-rPPG GitHub ì €ì¥ì†Œ í´ë¡  ì¤‘...")
                subprocess.run([
                    "git", "clone",
                    "https://github.com/ubicenter/ubfc-rppg.git",
                    str(ubfc_dir)
                ], check=True)
                logger.info("âœ… UBFC-rPPG ì €ì¥ì†Œ í´ë¡  ì™„ë£Œ")
            else:
                logger.info("âœ… UBFC-rPPG ì €ì¥ì†Œ ì´ë¯¸ ì¡´ì¬")
            
            # ë°ì´í„° êµ¬ì¡° í™•ì¸
            self._analyze_ubfc_structure(ubfc_dir)
            
        except Exception as e:
            logger.error(f"âŒ UBFC-rPPG ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
        
        return True
    
    def _analyze_vip_hr_structure(self, vip_dir: Path):
        """VIPL-HR ë°ì´í„° êµ¬ì¡° ë¶„ì„"""
        try:
            # ë°ì´í„° íŒŒì¼ ì°¾ê¸°
            video_files = list(vip_dir.rglob("*.avi")) + list(vip_dir.rglob("*.mp4"))
            metadata_files = list(vip_dir.rglob("*.txt")) + list(vip_dir.rglob("*.json"))
            
            logger.info(f"ğŸ“Š VIPL-HR ë°ì´í„° êµ¬ì¡° ë¶„ì„:")
            logger.info(f"  - ë¹„ë””ì˜¤ íŒŒì¼: {len(video_files)}ê°œ")
            logger.info(f"  - ë©”íƒ€ë°ì´í„°: {len(metadata_files)}ê°œ")
            
            # êµ¬ì¡° ì •ë³´ ì €ì¥
            structure_info = {
                "dataset": "VIPL-HR",
                "total_videos": len(video_files),
                "total_metadata": len(metadata_files),
                "video_extensions": list(set([f.suffix for f in video_files])),
                "metadata_extensions": list(set([f.suffix for f in metadata_files])),
                "analysis_date": str(Path().cwd())
            }
            
            structure_file = self.data_dir / "metadata" / "vip_hr_structure.json"
            with open(structure_file, 'w', encoding='utf-8') as f:
                json.dump(structure_info, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… VIPL-HR êµ¬ì¡° ì •ë³´ ì €ì¥: {structure_file}")
            
        except Exception as e:
            logger.error(f"âŒ VIPL-HR êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_ubfc_structure(self, ubfc_dir: Path):
        """UBFC-rPPG ë°ì´í„° êµ¬ì¡° ë¶„ì„"""
        try:
            # ë°ì´í„° íŒŒì¼ ì°¾ê¸°
            video_files = list(ubfc_dir.rglob("*.avi")) + list(ubfc_dir.rglob("*.mp4"))
            metadata_files = list(ubfc_dir.rglob("*.txt")) + list(ubfc_dir.rglob("*.json"))
            
            logger.info(f"ğŸ“Š UBFC-rPPG ë°ì´í„° êµ¬ì¡° ë¶„ì„:")
            logger.info(f"  - ë¹„ë””ì˜¤ íŒŒì¼: {len(video_files)}ê°œ")
            logger.info(f"  - ë©”íƒ€ë°ì´í„°: {len(metadata_files)}ê°œ")
            
            # êµ¬ì¡° ì •ë³´ ì €ì¥
            structure_info = {
                "dataset": "UBFC-rPPG",
                "total_videos": len(video_files),
                "total_metadata": len(metadata_files),
                "video_extensions": list(set([f.suffix for f in video_files])),
                "metadata_extensions": list(set([f.suffix for f in metadata_files])),
                "analysis_date": str(Path().cwd())
            }
            
            structure_file = self.data_dir / "metadata" / "ubfc_rppg_structure.json"
            with open(structure_file, 'w', encoding='utf-8') as f:
                json.dump(structure_info, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… UBFC-rPPG êµ¬ì¡° ì •ë³´ ì €ì¥: {structure_file}")
            
        except Exception as e:
            logger.error(f"âŒ UBFC-rPPG êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def create_data_integration_pipeline(self):
        """ë°ì´í„° í†µí•© íŒŒì´í”„ë¼ì¸ ìƒì„±"""
        logger.info("ğŸ”— ë°ì´í„° í†µí•© íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")
        
        try:
            pipeline_script = self.data_dir / "scripts" / "data_integration.py"
            pipeline_script.parent.mkdir(exist_ok=True)
            
            pipeline_content = """#!/usr/bin/env python3
\"\"\"
ë°ì´í„° í†µí•© íŒŒì´í”„ë¼ì¸
ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ í‘œì¤€í™”ëœ í¬ë§·ìœ¼ë¡œ í†µí•©
\"\"\"

import json
import pandas as pd
from pathlib import Path
import logging

def integrate_datasets():
    \"\"\"ë°ì´í„°ì…‹ í†µí•© ë©”ì¸ í•¨ìˆ˜\"\"\"
    # VIPL-HR, MAHNOB-HCI, UBFC-rPPG ë°ì´í„° í†µí•©
    # í‘œì¤€í™”ëœ í¬ë§·ìœ¼ë¡œ ë³€í™˜
    # í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„í• 
    pass

if __name__ == "__main__":
    integrate_datasets()
"""
            
            with open(pipeline_script, 'w', encoding='utf-8') as f:
                f.write(pipeline_content)
            
            logger.info(f"âœ… ë°ì´í„° í†µí•© íŒŒì´í”„ë¼ì¸ ìƒì„±: {pipeline_script}")
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def generate_data_summary(self):
        """ë°ì´í„° í˜„í™© ìš”ì•½ ìƒì„±"""
        logger.info("ğŸ“Š ë°ì´í„° í˜„í™© ìš”ì•½ ìƒì„± ì¤‘...")
        
        try:
            summary = {
                "operation_name": "ë¶ˆì‚¬ì¡° ì—”ì§„ ê³ ë„í™” v2 - ë°ì´í„° í™•ë³´",
                "phase": "Phase 0: ë°ì´í„° í™•ë³´",
                "target_datasets": self.datasets,
                "current_status": self.download_status,
                "data_directory": str(self.data_dir.absolute()),
                "next_phase": "Phase 1: ì •í™•ë„ 85% ë‹¬ì„± (ê³µê°œ ë°ì´í„° ê¸°ë°˜)",
                "estimated_total_samples": sum([ds["samples"] for ds in self.datasets.values()]),
                "estimated_total_size_gb": sum([ds["size_gb"] for ds in self.datasets.values()])
            }
            
            summary_file = self.data_dir / "metadata" / "phoenix_engine_data_summary.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ë°ì´í„° í˜„í™© ìš”ì•½ ìƒì„±: {summary_file}")
            
            # ìš”ì•½ ì¶œë ¥
            logger.info("=" * 60)
            logger.info("ğŸ† ë¶ˆì‚¬ì¡° ì—”ì§„ ë°ì´í„° í™•ë³´ í˜„í™©")
            logger.info("=" * 60)
            logger.info(f"ğŸ“Š ì´ ì˜ˆìƒ ìƒ˜í”Œ ìˆ˜: {summary['estimated_total_samples']:,}ê°œ")
            logger.info(f"ğŸ’¾ ì´ ì˜ˆìƒ í¬ê¸°: {summary['estimated_total_size_gb']:.1f}GB")
            logger.info(f"ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: {summary['next_phase']}")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def execute_phase_0(self):
        """Phase 0 ì‹¤í–‰ - ë°ì´í„° í™•ë³´"""
        logger.info("ğŸš€ Phase 0: ë°ì´í„° í™•ë³´ ì‘ì „ ê°œì‹œ!")
        logger.info("=" * 60)
        
        try:
            # 1. ë°ì´í„° ì €ì¥ êµ¬ì¡° ìƒì„±
            self.create_data_structure()
            
            # 2. VIPL-HR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
            logger.info("ğŸ”¥ VIPL-HR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            vip_success = self.download_vipl_hr_dataset()
            self.download_status["VIPL-HR"] = "success" if vip_success else "failed"
            
            # 3. MAHNOB-HCI ë°ì´í„°ì…‹ ê°€ì´ë“œ ìƒì„±
            logger.info("ğŸ¯ MAHNOB-HCI ë°ì´í„°ì…‹ ê°€ì´ë“œ ìƒì„±...")
            mahnob_success = self.download_mahnob_hci_dataset()
            self.download_status["MAHNOB-HCI"] = "guide_created" if mahnob_success else "failed"
            
            # 4. UBFC-rPPG ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
            logger.info("âš¡ UBFC-rPPG ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            ubfc_success = self.download_ubfc_rppg_dataset()
            self.download_status["UBFC-rPPG"] = "success" if ubfc_success else "failed"
            
            # 5. ë°ì´í„° í†µí•© íŒŒì´í”„ë¼ì¸ ìƒì„±
            self.create_data_integration_pipeline()
            
            # 6. ìµœì¢… ìš”ì•½ ìƒì„±
            self.generate_data_summary()
            
            logger.info("ğŸ‰ Phase 0: ë°ì´í„° í™•ë³´ ì‘ì „ ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ Phase 0 ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return False
        
        return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ ì‘ì „ëª…: 'ë¶ˆì‚¬ì¡° ì—”ì§„' ê³ ë„í™” v2 - ë°ì´í„° ìš°ì„ ì£¼ì˜")
    logger.info("ğŸ¯ ëª©í‘œ: ëŒ€ê·œëª¨ ê³µê°œ ë°ì´í„°ì…‹ í™•ë³´ë¡œ 95% ì´ìƒ ì •í™•ë„ ë‹¬ì„±")
    logger.info("=" * 60)
    
    try:
        # ë¶ˆì‚¬ì¡° ì—”ì§„ ë°ì´í„° í™•ë³´ ì‹œìŠ¤í…œ ìƒì„±
        phoenix_engine = PhoenixEngineDataAcquisition()
        
        # Phase 0 ì‹¤í–‰
        success = phoenix_engine.execute_phase_0()
        
        if success:
            logger.info("ğŸ† Phase 0 ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
            logger.info("ğŸš€ ë‹¤ìŒ ë‹¨ê³„: Phase 1 - ì •í™•ë„ 85% ë‹¬ì„±")
        else:
            logger.error("âŒ Phase 0 ì‹¤í–‰ ì‹¤íŒ¨")
            
    except Exception as e:
        logger.error(f"âŒ ë©”ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main() 