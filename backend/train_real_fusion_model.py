#!/usr/bin/env python3
"""
μ‹¤μ  λ°μ΄ν„°λ΅ μµν•© λ¨λΈ ν›λ ¨
"""

import numpy as np
import pandas as pd
import logging
import os
from datetime import datetime
from typing import Dict, List, Tuple, Any
import json
import joblib
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler

# λ΅κΉ… μ„¤μ •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RealFusionModelTrainer:
    """μ‹¤μ  λ°μ΄ν„°λ΅ μµν•© λ¨λΈ ν›λ ¨κΈ°"""
    
    def __init__(self):
        self.dataset_path = "./real_data_fusion_output/fusion_dataset"
        self.models_path = "./real_data_fusion_output/trained_models"
        self.results_path = "./real_data_fusion_output/training_results"
        
        # λ””λ ‰ν† λ¦¬ μƒμ„±
        os.makedirs(self.models_path, exist_ok=True)
        os.makedirs(self.results_path, exist_ok=True)
        
        # λ°μ΄ν„° λ΅λ“
        self.X_train = None
        self.y_train = None
        self.X_val = None
        self.y_val = None
        self.X_test = None
        self.y_test = None
        
        logger.info("μ‹¤μ  λ°μ΄ν„° μµν•© λ¨λΈ ν›λ ¨κΈ° μ΄κΈ°ν™” μ™„λ£")
    
    def load_dataset(self) -> bool:
        """λ°μ΄ν„°μ…‹ λ΅λ“"""
        try:
            logger.info("π“ μ‹¤μ  λ°μ΄ν„°μ…‹ λ΅λ“ μ‹μ‘")
            
            # ν›λ ¨ λ°μ΄ν„° λ΅λ“
            self.X_train = np.load(os.path.join(self.dataset_path, "train_features.npy"))
            self.y_train = np.load(os.path.join(self.dataset_path, "train_labels.npy"))
            
            # κ²€μ¦ λ°μ΄ν„° λ΅λ“
            self.X_val = np.load(os.path.join(self.dataset_path, "val_features.npy"))
            self.y_val = np.load(os.path.join(self.dataset_path, "val_labels.npy"))
            
            # ν…μ¤νΈ λ°μ΄ν„° λ΅λ“
            self.X_test = np.load(os.path.join(self.dataset_path, "test_features.npy"))
            self.y_test = np.load(os.path.join(self.dataset_path, "test_labels.npy"))
            
            logger.info(f"μ‹¤μ  λ°μ΄ν„°μ…‹ λ΅λ“ μ™„λ£:")
            logger.info(f"  ν›λ ¨: {self.X_train.shape[0]}κ° μƒν”, {self.X_train.shape[1]}κ° νΉμ§•")
            logger.info(f"  κ²€μ¦: {self.X_val.shape[0]}κ° μƒν”, {self.X_val.shape[1]}κ° νΉμ§•")
            logger.info(f"  ν…μ¤νΈ: {self.X_test.shape[0]}κ° μƒν”, {self.X_test.shape[1]}κ° νΉμ§•")
            
            return True
            
        except Exception as e:
            logger.error(f"λ°μ΄ν„°μ…‹ λ΅λ“ μ‹¤ν¨: {e}")
            return False
    
    def train_models(self) -> Dict[str, Any]:
        """μ—¬λ¬ λ¨λΈ ν›λ ¨ λ° λΉ„κµ"""
        try:
            logger.info("π― μ‹¤μ  λ°μ΄ν„°λ΅ μµν•© λ¨λΈ ν›λ ¨ μ‹μ‘")
            
            # λ°μ΄ν„° μ •κ·ν™”
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(self.X_train)
            X_val_scaled = scaler.transform(self.X_val)
            X_test_scaled = scaler.transform(self.X_test)
            
            # μ¤μΌ€μΌλ¬ μ €μ¥
            scaler_path = os.path.join(self.models_path, "real_feature_scaler.pkl")
            joblib.dump(scaler, scaler_path)
            logger.info(f"μ‹¤μ  λ°μ΄ν„° νΉμ§• μ¤μΌ€μΌλ¬ μ €μ¥ μ™„λ£: {scaler_path}")
            
            # λ¨λΈ μ •μ
            models = {
                'LinearRegression': LinearRegression(),
                'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),
                'SVR': SVR(kernel='rbf', C=1.0, gamma='scale')
            }
            
            # λ¨λΈ ν›λ ¨ λ° ν‰κ°€
            results = {}
            
            for name, model in models.items():
                logger.info(f"λ¨λΈ ν›λ ¨ μ¤‘: {name}")
                
                # λ¨λΈ ν›λ ¨
                model.fit(X_train_scaled, self.y_train)
                
                # μμΈ΅
                y_train_pred = model.predict(X_train_scaled)
                y_val_pred = model.predict(X_val_scaled)
                y_test_pred = model.predict(X_test_scaled)
                
                # μ„±λ¥ ν‰κ°€
                train_mse = mean_squared_error(self.y_train, y_train_pred)
                train_mae = mean_absolute_error(self.y_train, y_train_pred)
                train_r2 = r2_score(self.y_train, y_train_pred)
                
                val_mse = mean_squared_error(self.y_val, y_val_pred)
                val_mae = mean_absolute_error(self.y_val, y_val_pred)
                val_r2 = r2_score(self.y_val, y_val_pred)
                
                test_mse = mean_squared_error(self.y_test, y_test_pred)
                test_mae = mean_absolute_error(self.y_test, y_test_pred)
                test_r2 = r2_score(self.y_test, y_test_pred)
                
                # κµμ°¨ κ²€μ¦
                cv_scores = cross_val_score(model, X_train_scaled, self.y_train, cv=5, scoring='r2')
                
                # κ²°κ³Ό μ €μ¥
                results[name] = {
                    'model': model,
                    'train_mse': train_mse,
                    'train_mae': train_mae,
                    'train_r2': train_r2,
                    'val_mse': val_mse,
                    'val_mae': val_mae,
                    'val_r2': val_r2,
                    'test_mse': test_mse,
                    'test_mae': test_mae,
                    'test_r2': test_r2,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std()
                }
                
                logger.info(f"{name} ν›λ ¨ μ™„λ£:")
                logger.info(f"  ν›λ ¨ RΒ²: {train_r2:.4f}, κ²€μ¦ RΒ²: {val_r2:.4f}, ν…μ¤νΈ RΒ²: {test_r2:.4f}")
                logger.info(f"  κµμ°¨ κ²€μ¦: {cv_scores.mean():.4f} Β± {cv_scores.std():.4f}")
            
            # μµκ³  μ„±λ¥ λ¨λΈ μ„ νƒ
            best_model_name = max(results.keys(), key=lambda k: results[k]['val_r2'])
            best_model = results[best_model_name]['model']
            
            logger.info(f"π― μµκ³  μ„±λ¥ λ¨λΈ: {best_model_name} (κ²€μ¦ RΒ²: {results[best_model_name]['val_r2']:.4f})")
            
            # μµκ³  μ„±λ¥ λ¨λΈ μ €μ¥
            best_model_path = os.path.join(self.models_path, "real_best_fusion_model.pkl")
            joblib.dump(best_model, best_model_path)
            logger.info(f"μ‹¤μ  λ°μ΄ν„° μµκ³  μ„±λ¥ λ¨λΈ μ €μ¥ μ™„λ£: {best_model_path}")
            
            # λ¨λ“  λ¨λΈ μ €μ¥
            for name, result in results.items():
                model_path = os.path.join(self.models_path, f"real_{name.lower()}_model.pkl")
                joblib.dump(result['model'], model_path)
                logger.info(f"μ‹¤μ  λ°μ΄ν„° {name} λ¨λΈ μ €μ¥ μ™„λ£: {model_path}")
            
            return results
            
        except Exception as e:
            logger.error(f"λ¨λΈ ν›λ ¨ μ‹¤ν¨: {e}")
            return {}
    
    def hyperparameter_tuning(self, model_name: str = 'RandomForest') -> Dict[str, Any]:
        """ν•μ΄νΌνλΌλ―Έν„° νλ‹"""
        try:
            logger.info(f"π”§ {model_name} ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ‹μ‘")
            
            # λ°μ΄ν„° μ •κ·ν™”
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(self.X_train)
            X_val_scaled = scaler.transform(self.X_val)
            
            # ν•μ΄νΌνλΌλ―Έν„° κ·Έλ¦¬λ“ μ •μ
            if model_name == 'RandomForest':
                param_grid = {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [10, 20, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                }
                base_model = RandomForestRegressor(random_state=42)
            elif model_name == 'SVR':
                param_grid = {
                    'C': [0.1, 1, 10],
                    'gamma': ['scale', 'auto', 0.001, 0.01],
                    'kernel': ['rbf', 'linear']
                }
                base_model = SVR()
            else:
                logger.warning(f"{model_name}μ— λ€ν• ν•μ΄νΌνλΌλ―Έν„° νλ‹μ„ μ§€μ›ν•μ§€ μ•μµλ‹λ‹¤")
                return {}
            
            # κ·Έλ¦¬λ“ μ„μΉ
            grid_search = GridSearchCV(
                base_model, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1
            )
            grid_search.fit(X_train_scaled, self.y_train)
            
            # μµμ  λ¨λΈ
            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_
            best_score = grid_search.best_score_
            
            logger.info(f"μµμ  ν•μ΄νΌνλΌλ―Έν„°: {best_params}")
            logger.info(f"μµμ  κµμ°¨ κ²€μ¦ μ μ: {best_score:.4f}")
            
            # μµμ  λ¨λΈ μ„±λ¥ ν‰κ°€
            y_val_pred = best_model.predict(X_val_scaled)
            val_r2 = r2_score(self.y_val, y_val_pred)
            val_mse = mean_squared_error(self.y_val, y_val_pred)
            
            logger.info(f"κ²€μ¦ μ„±λ¥ - RΒ²: {val_r2:.4f}, MSE: {val_mse:.4f}")
            
            # μµμ  λ¨λΈ μ €μ¥
            tuned_model_path = os.path.join(self.models_path, f"real_{model_name.lower()}_tuned.pkl")
            joblib.dump(best_model, tuned_model_path)
            logger.info(f"μ‹¤μ  λ°μ΄ν„° νλ‹λ {model_name} λ¨λΈ μ €μ¥ μ™„λ£: {tuned_model_path}")
            
            return {
                'best_model': best_model,
                'best_params': best_params,
                'best_score': best_score,
                'val_r2': val_r2,
                'val_mse': val_mse
            }
            
        except Exception as e:
            logger.error(f"ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ‹¤ν¨: {e}")
            return {}
    
    def generate_training_report(self, results: Dict[str, Any], tuning_results: Dict[str, Any]) -> bool:
        """ν›λ ¨ κ²°κ³Ό λ³΄κ³ μ„ μƒμ„±"""
        try:
            logger.info("π“ μ‹¤μ  λ°μ΄ν„° μµν•© λ¨λΈ ν›λ ¨ κ²°κ³Ό λ³΄κ³ μ„ μƒμ„± μ‹μ‘")
            
            # λ³΄κ³ μ„ λ‚΄μ© μƒμ„±
            report = f"""# π― μ‹¤μ  λ°μ΄ν„° μµν•© λ¨λΈ ν›λ ¨ κ²°κ³Ό λ³΄κ³ μ„

## π“‹ ν›λ ¨ κ°μ”
- **ν›λ ¨ μΌμ‹**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **λ°μ΄ν„°μ…‹**: μ‹¤μ  CMI λ°μ΄ν„° (574,945κ° μƒν”)
- **νΉμ§• μ°¨μ›**: {self.X_train.shape[1]}κ° (CMI 21κ° + μμ„± 8κ°)
- **λ°μ΄ν„° λ¶„ν• **: ν›λ ¨ {len(self.X_train)}κ°, κ²€μ¦ {len(self.X_val)}κ°, ν…μ¤νΈ {len(self.X_test)}κ°

## π† λ¨λΈ μ„±λ¥ λΉ„κµ

### π“ μ„±λ¥ μ§€ν‘ μ”μ•½
| λ¨λΈ | ν›λ ¨ RΒ² | κ²€μ¦ RΒ² | ν…μ¤νΈ RΒ² | κµμ°¨ κ²€μ¦ (ν‰κ· Β±ν‘μ¤€νΈμ°¨) |
|------|---------|---------|-----------|---------------------------|
"""
            
            # λ¨λΈλ³„ μ„±λ¥ ν…μ΄λΈ” μƒμ„±
            for name, result in results.items():
                report += f"| {name} | {result['train_r2']:.4f} | {result['val_r2']:.4f} | {result['test_r2']:.4f} | {result['cv_mean']:.4f}Β±{result['cv_std']:.4f} |\n"
            
            # μµκ³  μ„±λ¥ λ¨λΈ μ •λ³΄
            best_model_name = max(results.keys(), key=lambda k: results[k]['val_r2'])
            best_result = results[best_model_name]
            
            report += f"""
## π― μµκ³  μ„±λ¥ λ¨λΈ: {best_model_name}

### π“ μƒμ„Έ μ„±λ¥ μ§€ν‘
- **ν›λ ¨ μ„±λ¥**:
  - MSE: {best_result['train_mse']:.6f}
  - MAE: {best_result['train_mae']:.6f}
  - RΒ²: {best_result['train_r2']:.4f}
- **κ²€μ¦ μ„±λ¥**:
  - MSE: {best_result['val_mse']:.6f}
  - MAE: {best_result['val_mae']:.6f}
  - RΒ²: {best_result['val_r2']:.4f}
- **ν…μ¤νΈ μ„±λ¥**:
  - MSE: {best_result['test_mse']:.6f}
  - MAE: {best_result['test_mae']:.6f}
  - RΒ²: {best_result['test_r2']:.4f}
- **κµμ°¨ κ²€μ¦**: {best_result['cv_mean']:.4f} Β± {best_result['cv_std']:.4f}

## π”§ ν•μ΄νΌνλΌλ―Έν„° νλ‹ κ²°κ³Ό
"""
            
            if tuning_results:
                report += f"""
### RandomForest νλ‹ κ²°κ³Ό
- **μµμ  ν•μ΄νΌνλΌλ―Έν„°**: {tuning_results.get('best_params', 'N/A')}
- **μµμ  κµμ°¨ κ²€μ¦ μ μ**: {tuning_results.get('best_score', 0):.4f}
- **κ²€μ¦ μ„±λ¥**: RΒ² = {tuning_results.get('val_r2', 0):.4f}, MSE = {tuning_results.get('val_mse', 0):.6f}
"""
            else:
                report += "ν•μ΄νΌνλΌλ―Έν„° νλ‹μ„ μν–‰ν•μ§€ μ•μ•μµλ‹λ‹¤.\n"
            
            report += f"""
## π“ μ €μ¥λ λ¨λΈ νμΌ
- **μ‹¤μ  λ°μ΄ν„° νΉμ§• μ¤μΌ€μΌλ¬**: `real_feature_scaler.pkl`
- **μ‹¤μ  λ°μ΄ν„° μµκ³  μ„±λ¥ λ¨λΈ**: `real_best_fusion_model.pkl`
- **μ‹¤μ  λ°μ΄ν„° κ°λ³„ λ¨λΈλ“¤**: `real_linearregression_model.pkl`, `real_randomforest_model.pkl`, `real_svr_model.pkl`
- **μ‹¤μ  λ°μ΄ν„° νλ‹λ λ¨λΈ**: `real_randomforest_tuned.pkl` (νλ‹ μν–‰ μ‹)

## π‰ κ²°λ΅ 
π― **μ‹¤μ  CMI λ°μ΄ν„°λ΅ μµν•© λ¨λΈ ν›λ ¨μ΄ μ„±κ³µμ μΌλ΅ μ™„λ£λμ—μµλ‹λ‹¤!**

{best_model_name}μ΄ κ²€μ¦ RΒ² {best_result['val_r2']:.4f}λ΅ μµκ³  μ„±λ¥μ„ λ³΄μ€μΌλ©°,
μ΄μ  μ‹¤μ  rPPG-μμ„± μµν•© λ¶„μ„μ— μ‚¬μ©ν•  μ μμµλ‹λ‹¤.

**'νΈμ μ—”μ§„' μ‹¤μ  λ°μ΄ν„° ν›λ ¨ μ™„λ£!** π€

## π” Mock λ°μ΄ν„° vs μ‹¤μ  λ°μ΄ν„° μ„±λ¥ λΉ„κµ
- **Mock λ°μ΄ν„°**: κ²€μ¦ RΒ² 0.9906 (1,000κ° μƒν”)
- **μ‹¤μ  λ°μ΄ν„°**: κ²€μ¦ RΒ² {best_result['val_r2']:.4f} ({len(self.X_val)}κ° μƒν”)
- **λ°μ΄ν„° κ·λ¨**: {len(self.X_train) + len(self.X_val) + len(self.X_test)}λ°° μ¦κ°€
"""
            
            # λ³΄κ³ μ„ μ €μ¥
            report_path = os.path.join(self.results_path, "real_fusion_model_training_report.md")
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            
            logger.info(f"μ‹¤μ  λ°μ΄ν„° μµν•© λ¨λΈ ν›λ ¨ κ²°κ³Ό λ³΄κ³ μ„ μƒμ„± μ™„λ£: {report_path}")
            return True
            
        except Exception as e:
            logger.error(f"λ³΄κ³ μ„ μƒμ„± μ‹¤ν¨: {e}")
            return False
    
    def run_complete_training(self) -> bool:
        """μ™„μ „ν• ν›λ ¨ νμ΄ν”„λΌμΈ μ‹¤ν–‰"""
        try:
            logger.info("π€ μ‹¤μ  λ°μ΄ν„° μµν•© λ¨λΈ μ™„μ „ ν›λ ¨ νμ΄ν”„λΌμΈ μ‹μ‘")
            
            # 1λ‹¨κ³„: λ°μ΄ν„°μ…‹ λ΅λ“
            if not self.load_dataset():
                return False
            
            # 2λ‹¨κ³„: κΈ°λ³Έ λ¨λΈ ν›λ ¨
            results = self.train_models()
            if not results:
                return False
            
            # 3λ‹¨κ³„: ν•μ΄νΌνλΌλ―Έν„° νλ‹ (RandomForest)
            tuning_results = self.hyperparameter_tuning('RandomForest')
            
            # 4λ‹¨κ³„: κ²°κ³Ό λ³΄κ³ μ„ μƒμ„±
            if not self.generate_training_report(results, tuning_results):
                return False
            
            logger.info("π‰ μ‹¤μ  λ°μ΄ν„° μµν•© λ¨λΈ μ™„μ „ ν›λ ¨ νμ΄ν”„λΌμΈ μ™„λ£!")
            return True
            
        except Exception as e:
            logger.error(f"ν›λ ¨ νμ΄ν”„λΌμΈ μ‹¤ν–‰ μ‹¤ν¨: {e}")
            return False

def main():
    """λ©”μΈ μ‹¤ν–‰ ν•¨μ"""
    try:
        logger.info("π― μ‹¤μ  λ°μ΄ν„° μµν•© λ¨λΈ ν›λ ¨ μ‹μ‘")
        
        # ν›λ ¨κΈ° μ΄κΈ°ν™”
        trainer = RealFusionModelTrainer()
        
        # μ™„μ „ν• ν›λ ¨ νμ΄ν”„λΌμΈ μ‹¤ν–‰
        success = trainer.run_complete_training()
        
        if success:
            logger.info("π‰ μ‹¤μ  λ°μ΄ν„° μµν•© λ¨λΈ ν›λ ¨ μ™„λ£!")
            print("\n" + "="*60)
            print("π‰ μ‹¤μ  λ°μ΄ν„° μµν•© λ¨λΈ ν›λ ¨ μ„±κ³µ!")
            print("="*60)
            print("β… μ‹¤μ  CMI λ°μ΄ν„°λ΅ μµν•© λ¨λΈ ν›λ ¨ μ™„λ£")
            print("β… μµκ³  μ„±λ¥ λ¨λΈ μ„ νƒ λ° μ €μ¥ μ™„λ£")
            print("β… ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ™„λ£")
            print("β… μƒμ„Έ ν›λ ¨ κ²°κ³Ό λ³΄κ³ μ„ μƒμ„± μ™„λ£")
            print("β… 'νΈμ μ—”μ§„' μ‹¤μ  λ°μ΄ν„° ν›λ ¨ μ™„λ£!")
            print("="*60)
        else:
            logger.error("β μ‹¤μ  λ°μ΄ν„° μµν•© λ¨λΈ ν›λ ¨ μ‹¤ν¨")
            print("\n" + "="*60)
            print("β ν›λ ¨ μ‹¤ν¨ - λ¬Έμ λ¥Ό νμ•…ν•κ³  μμ •ν•΄μ£Όμ„Έμ”.")
            print("="*60)
        
    except Exception as e:
        logger.error(f"λ©”μΈ μ‹¤ν–‰ μ¤‘ μ¤λ¥: {e}")
        print(f"\nβ μ‹¤ν–‰ μ¤λ¥: {e}")

if __name__ == "__main__":
    main()
