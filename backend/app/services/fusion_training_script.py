#!/usr/bin/env python3
"""
ìœµí•© ëª¨ë¸ í›ˆë ¨ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
"""

import numpy as np
import logging
import os
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any

# ê¸°ì¡´ ì„œë¹„ìŠ¤ë“¤ import
from .fusion_data_pipeline import FusionDataPipeline
from .fusion_analyzer import AdvancedFusionAnalyzer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class FusionModelTrainer:
    """
    ìœµí•© ëª¨ë¸ í›ˆë ¨ ë©”ì¸ í´ë˜ìŠ¤
    
    í•µì‹¬ ê¸°ëŠ¥:
    1. ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    2. ëª¨ë¸ í›ˆë ¨ ë° ê²€ì¦
    3. ì„±ëŠ¥ í‰ê°€ ë° ëª¨ë¸ ì €ì¥
    4. í›ˆë ¨ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
    """
    
    def __init__(self):
        # íŒŒì´í”„ë¼ì¸ ë° ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.data_pipeline = FusionDataPipeline()
        self.fusion_analyzer = AdvancedFusionAnalyzer()
        
        # í›ˆë ¨ ì„¤ì •
        self.training_config = {
            'model_save_path': './models/fusion_model.joblib',
            'results_save_path': './results/training_results.json',
            'min_training_samples': 50,
            'target_accuracy': 0.85,
            'max_training_iterations': 3
        }
        
        # í›ˆë ¨ ê²°ê³¼
        self.training_results = {}
        
        logger.info("ìœµí•© ëª¨ë¸ í›ˆë ¨ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def run_complete_training_pipeline(
        self,
        cmi_data_path: str,
        voice_dataset_path: str,
        output_base_path: str
    ) -> bool:
        """
        ì™„ì „í•œ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            cmi_data_path: CMI ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            voice_dataset_path: ìŒì„± ë°ì´í„°ì…‹ ê²½ë¡œ
            output_base_path: ì¶œë ¥ ê¸°ë³¸ ê²½ë¡œ
            
        Returns:
            í›ˆë ¨ ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info("ğŸš€ ì™„ì „í•œ ìœµí•© ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
            
            # 1ë‹¨ê³„: ë°ì´í„° í†µí•© ë° ì „ì²˜ë¦¬
            logger.info("ğŸ“Š 1ë‹¨ê³„: ë°ì´í„° í†µí•© ë° ì „ì²˜ë¦¬")
            data_integration_success = self._integrate_data(
                cmi_data_path, voice_dataset_path, output_base_path
            )
            
            if not data_integration_success:
                logger.error("ë°ì´í„° í†µí•© ì‹¤íŒ¨")
                return False
            
            # 2ë‹¨ê³„: í›ˆë ¨ ë°ì´í„° ê²€ì¦
            logger.info("ğŸ” 2ë‹¨ê³„: í›ˆë ¨ ë°ì´í„° ê²€ì¦")
            if not self._validate_training_data():
                logger.error("í›ˆë ¨ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨")
                return False
            
            # 3ë‹¨ê³„: ëª¨ë¸ í›ˆë ¨
            logger.info("ğŸ¯ 3ë‹¨ê³„: ëª¨ë¸ í›ˆë ¨")
            training_success = self._train_fusion_model()
            
            if not training_success:
                logger.error("ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨")
                return False
            
            # 4ë‹¨ê³„: ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
            logger.info("ğŸ“ˆ 4ë‹¨ê³„: ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
            evaluation_success = self._evaluate_model_performance()
            
            if not evaluation_success:
                logger.error("ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨")
                return False
            
            # 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ë³´ê³ ì„œ ìƒì„±
            logger.info("ğŸ’¾ 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ë³´ê³ ì„œ ìƒì„±")
            self._save_training_results(output_base_path)
            self._generate_training_report(output_base_path)
            
            logger.info("âœ… ìœµí•© ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return False
    
    def _integrate_data(
        self,
        cmi_data_path: str,
        voice_dataset_path: str,
        output_base_path: str
    ) -> bool:
        """ë°ì´í„° í†µí•© ì‹¤í–‰"""
        try:
            # ë°ì´í„° í†µí•© ì‹¤í–‰
            success = self.data_pipeline.integrate_cmi_and_voice_data(
                cmi_data_path=cmi_data_path,
                voice_dataset_path=voice_dataset_path,
                output_path=f"{output_base_path}/integrated_dataset"
            )
            
            if success:
                logger.info("ë°ì´í„° í†µí•© ì„±ê³µ")
                return True
            else:
                logger.error("ë°ì´í„° í†µí•© ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            logger.error(f"ë°ì´í„° í†µí•© ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _validate_training_data(self) -> bool:
        """í›ˆë ¨ ë°ì´í„° ê²€ì¦"""
        try:
            # í›ˆë ¨ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            train_data, val_data, test_data = self.data_pipeline.get_training_data()
            
            # ë°ì´í„° ìˆ˜ëŸ‰ ê²€ì¦
            if len(train_data) < self.training_config['min_training_samples']:
                logger.error(f"í›ˆë ¨ ë°ì´í„° ë¶€ì¡±: {len(train_data)}ê°œ (ìµœì†Œ {self.training_config['min_training_samples']}ê°œ í•„ìš”)")
                return False
            
            # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            if not self._check_data_quality(train_data):
                logger.error("í›ˆë ¨ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨")
                return False
            
            logger.info(f"í›ˆë ¨ ë°ì´í„° ê²€ì¦ ì„±ê³µ: í›ˆë ¨ {len(train_data)}ê°œ, ê²€ì¦ {len(val_data)}ê°œ, í…ŒìŠ¤íŠ¸ {len(test_data)}ê°œ")
            return True
            
        except Exception as e:
            logger.error(f"í›ˆë ¨ ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _check_data_quality(self, training_data: List[Tuple[np.ndarray, float]]) -> bool:
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        try:
            if not training_data:
                return False
            
            # íŠ¹ì§• ì°¨ì› ê²€ì¦
            feature_dim = len(training_data[0][0])
            if feature_dim != 18:  # rPPG(10) + ìŒì„±(8)
                logger.error(f"íŠ¹ì§• ì°¨ì› ë¶ˆì¼ì¹˜: {feature_dim} (ì˜ˆìƒ: 18)")
                return False
            
            # ë¼ë²¨ ë²”ìœ„ ê²€ì¦
            labels = [label for _, label in training_data]
            if min(labels) < 0 or max(labels) > 1:
                logger.error(f"ë¼ë²¨ ë²”ìœ„ ì˜¤ë¥˜: {min(labels)} ~ {max(labels)} (ì˜ˆìƒ: 0~1)")
                return False
            
            # NaN/Inf ê²€ì¦
            for features, _ in training_data:
                if np.any(np.isnan(features)) or np.any(np.isinf(features)):
                    logger.error("íŠ¹ì§•ì— NaN ë˜ëŠ” Inf ê°’ ë°œê²¬")
                    return False
            
            logger.info("ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í†µê³¼")
            return True
            
        except Exception as e:
            logger.error(f"ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _train_fusion_model(self) -> bool:
        """ìœµí•© ëª¨ë¸ í›ˆë ¨"""
        try:
            # í›ˆë ¨ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            train_data, val_data, test_data = self.data_pipeline.get_training_data()
            
            # í›ˆë ¨ ë°˜ë³µ (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´)
            best_accuracy = 0.0
            best_iteration = 0
            
            for iteration in range(self.training_config['max_training_iterations']):
                logger.info(f"ğŸ”„ í›ˆë ¨ ë°˜ë³µ {iteration + 1}/{self.training_config['max_training_iterations']}")
                
                # ëª¨ë¸ í›ˆë ¨
                training_success = self.fusion_analyzer.train_fusion_model(train_data)
                
                if not training_success:
                    logger.error(f"í›ˆë ¨ ë°˜ë³µ {iteration + 1} ì‹¤íŒ¨")
                    continue
                
                # ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€
                accuracy = self._evaluate_on_validation_data(val_data)
                
                logger.info(f"í›ˆë ¨ ë°˜ë³µ {iteration + 1} ì •í™•ë„: {accuracy:.4f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if accuracy > best_accuracy:
                    best_accuracy = accuracy
                    best_iteration = iteration + 1
                    
                    # ëª¨ë¸ ì €ì¥
                    self._save_best_model()
                
                # ëª©í‘œ ì •í™•ë„ ë‹¬ì„± ì‹œ ì¡°ê¸° ì¢…ë£Œ
                if accuracy >= self.training_config['target_accuracy']:
                    logger.info(f"ğŸ¯ ëª©í‘œ ì •í™•ë„ ë‹¬ì„±: {accuracy:.4f}")
                    break
            
            # ìµœì¢… ê²°ê³¼ ì €ì¥
            self.training_results['best_accuracy'] = best_accuracy
            self.training_results['best_iteration'] = best_iteration
            self.training_results['total_iterations'] = iteration + 1
            
            logger.info(f"ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: ìµœê³  ì •í™•ë„ {best_accuracy:.4f} (ë°˜ë³µ {best_iteration})")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _evaluate_on_validation_data(self, val_data: List[Tuple[np.ndarray, float]]) -> float:
        """ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€"""
        try:
            if not val_data:
                return 0.0
            
            correct_predictions = 0
            total_predictions = 0
            
            for features, true_label in val_data:
                try:
                    # ëª¨ë¸ ì˜ˆì¸¡
                    prediction = self.fusion_analyzer.fusion_model.predict(features.reshape(1, -1))[0]
                    
                    # ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚° (0.1 ì´ë‚´ ì˜¤ì°¨ë¥¼ ì •í™•í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼)
                    if abs(prediction - true_label) <= 0.1:
                        correct_predictions += 1
                    
                    total_predictions += 1
                    
                except Exception as e:
                    logger.warning(f"ê²€ì¦ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    continue
            
            accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0.0
            return accuracy
            
        except Exception as e:
            logger.error(f"ê²€ì¦ ë°ì´í„° í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _evaluate_model_performance(self) -> bool:
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        try:
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìµœì¢… ì„±ëŠ¥ í‰ê°€
            _, _, test_data = self.data_pipeline.get_training_data()
            
            if not test_data:
                logger.warning("í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return True
            
            # ì •í™•ë„ í‰ê°€
            test_accuracy = self._evaluate_on_validation_data(test_data)
            
            # ìƒì„¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­
            performance_metrics = self._calculate_detailed_metrics(test_data)
            
            # ê²°ê³¼ ì €ì¥
            self.training_results['test_accuracy'] = test_accuracy
            self.training_results['performance_metrics'] = performance_metrics
            
            logger.info(f"ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f}")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _calculate_detailed_metrics(self, test_data: List[Tuple[np.ndarray, float]]) -> Dict[str, Any]:
        """ìƒì„¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            predictions = []
            true_labels = []
            
            for features, true_label in test_data:
                try:
                    prediction = self.fusion_analyzer.fusion_model.predict(features.reshape(1, -1))[0]
                    predictions.append(prediction)
                    true_labels.append(true_label)
                except Exception as e:
                    continue
            
            if not predictions:
                return {"error": "ì˜ˆì¸¡ ì‹¤íŒ¨"}
            
            # ê¸°ë³¸ í†µê³„
            predictions = np.array(predictions)
            true_labels = np.array(true_labels)
            
            # ì˜¤ì°¨ ê³„ì‚°
            errors = np.abs(predictions - true_labels)
            
            metrics = {
                'mean_absolute_error': float(np.mean(errors)),
                'mean_squared_error': float(np.mean(errors ** 2)),
                'root_mean_squared_error': float(np.sqrt(np.mean(errors ** 2))),
                'mean_absolute_percentage_error': float(np.mean(np.abs(errors / (true_labels + 1e-8))) * 100),
                'r_squared': float(self._calculate_r_squared(true_labels, predictions)),
                'correlation': float(np.corrcoef(true_labels, predictions)[0, 1])
            }
            
            return metrics
            
        except Exception as e:
            logger.error(f"ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def _calculate_r_squared(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """RÂ² ì ìˆ˜ ê³„ì‚°"""
        try:
            ss_res = np.sum((y_true - y_pred) ** 2)
            ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
            r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
            return r2
        except Exception:
            return 0.0
    
    def _save_best_model(self) -> bool:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥"""
        try:
            # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(self.training_config['model_save_path']), exist_ok=True)
            
            # ëª¨ë¸ ì €ì¥
            success = self.fusion_analyzer.save_model(self.training_config['model_save_path'])
            
            if success:
                logger.info(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.training_config['model_save_path']}")
                return True
            else:
                logger.error("ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _save_training_results(self, output_base_path: str) -> bool:
        """í›ˆë ¨ ê²°ê³¼ ì €ì¥"""
        try:
            # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            results_dir = f"{output_base_path}/training_results"
            os.makedirs(results_dir, exist_ok=True)
            
            # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
            results_file = f"{results_dir}/training_results.json"
            
            # ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            final_results = {
                'training_config': self.training_config,
                'training_results': self.training_results,
                'dataset_info': self.data_pipeline.get_dataset_info(),
                'training_timestamp': datetime.now().isoformat(),
                'model_path': self.training_config['model_save_path']
            }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(final_results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"í›ˆë ¨ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_file}")
            return True
            
        except Exception as e:
            logger.error(f"í›ˆë ¨ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _generate_training_report(self, output_base_path: str) -> bool:
        """í›ˆë ¨ ë³´ê³ ì„œ ìƒì„±"""
        try:
            # ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ìƒì„±
            report_dir = f"{output_base_path}/reports"
            os.makedirs(report_dir, exist_ok=True)
            
            # ë³´ê³ ì„œ íŒŒì¼ ê²½ë¡œ
            report_file = f"{report_dir}/training_report.md"
            
            # Markdown ë³´ê³ ì„œ ìƒì„±
            report_content = self._create_markdown_report()
            
            # íŒŒì¼ ì €ì¥
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            logger.info(f"í›ˆë ¨ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_file}")
            return True
            
        except Exception as e:
            logger.error(f"í›ˆë ¨ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _create_markdown_report(self) -> str:
        """Markdown í˜•ì‹ì˜ í›ˆë ¨ ë³´ê³ ì„œ ìƒì„±"""
        try:
            dataset_info = self.data_pipeline.get_dataset_info()
            
            report = f"""# ğŸ¯ ìœµí•© ëª¨ë¸ í›ˆë ¨ ë³´ê³ ì„œ

## ğŸ“‹ í›ˆë ¨ ê°œìš”
- **í›ˆë ¨ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ëª¨ë¸ ìœ í˜•**: rPPG-ìŒì„± ìœµí•© ëª¨ë¸
- **í›ˆë ¨ ì•Œê³ ë¦¬ì¦˜**: Random Forest Ensemble

## ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´
- **ì´ ìƒ˜í”Œ ìˆ˜**: {dataset_info.get('training_samples', 0) + dataset_info.get('validation_samples', 0) + dataset_info.get('test_samples', 0)}
- **í›ˆë ¨ ìƒ˜í”Œ**: {dataset_info.get('training_samples', 0)}ê°œ
- **ê²€ì¦ ìƒ˜í”Œ**: {dataset_info.get('validation_samples', 0)}ê°œ
- **í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ**: {dataset_info.get('test_samples', 0)}ê°œ
- **íŠ¹ì§• ì°¨ì›**: {dataset_info.get('feature_dimension', 0)}ê°œ

## ğŸ¯ í›ˆë ¨ ê²°ê³¼
- **ìµœê³  ì •í™•ë„**: {self.training_results.get('best_accuracy', 0.0):.4f}
- **ìµœê³  ì„±ëŠ¥ ë°˜ë³µ**: {self.training_results.get('best_iteration', 0)}íšŒì°¨
- **ì´ í›ˆë ¨ ë°˜ë³µ**: {self.training_results.get('total_iterations', 0)}íšŒ
- **í…ŒìŠ¤íŠ¸ ì •í™•ë„**: {self.training_results.get('test_accuracy', 0.0):.4f}

## ğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­
"""
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ê°€
            if 'performance_metrics' in self.training_results:
                metrics = self.training_results['performance_metrics']
                if 'error' not in metrics:
                    report += f"""
- **í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE)**: {metrics.get('mean_absolute_error', 0.0):.4f}
- **í‰ê·  ì œê³± ì˜¤ì°¨ (MSE)**: {metrics.get('mean_squared_error', 0.0):.4f}
- **ë£¨íŠ¸ í‰ê·  ì œê³± ì˜¤ì°¨ (RMSE)**: {metrics.get('root_mean_squared_error', 0.0):.4f}
- **í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨ (MAPE)**: {metrics.get('mean_absolute_percentage_error', 0.0):.2f}%
- **RÂ² ì ìˆ˜**: {metrics.get('r_squared', 0.0):.4f}
- **ìƒê´€ê³„ìˆ˜**: {metrics.get('correlation', 0.0):.4f}
"""
            
            report += f"""
## ğŸ’¾ ëª¨ë¸ ì •ë³´
- **ëª¨ë¸ ì €ì¥ ê²½ë¡œ**: {self.training_config['model_save_path']}
- **ê²°ê³¼ ì €ì¥ ê²½ë¡œ**: {self.training_config['results_save_path']}

## ğŸ‰ í›ˆë ¨ ì™„ë£Œ
ìœµí•© ëª¨ë¸ í›ˆë ¨ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.
"""
            
            return report
            
        except Exception as e:
            logger.error(f"Markdown ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return f"# ì˜¤ë¥˜\n\në³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}"
    
    def get_training_summary(self) -> Dict[str, Any]:
        """í›ˆë ¨ ìš”ì•½ ë°˜í™˜"""
        return {
            'status': 'completed' if self.training_results else 'not_started',
            'best_accuracy': self.training_results.get('best_accuracy', 0.0),
            'test_accuracy': self.training_results.get('test_accuracy', 0.0),
            'model_saved': os.path.exists(self.training_config['model_save_path']),
            'dataset_info': self.data_pipeline.get_dataset_info()
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        logger.info("ğŸ¯ ìœµí•© ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
        
        # í›ˆë ¨ê¸° ì´ˆê¸°í™”
        trainer = FusionModelTrainer()
        
        # ì˜ˆì‹œ ë°ì´í„° ê²½ë¡œ (ì‹¤ì œ ì‚¬ìš© ì‹œ ìˆ˜ì • í•„ìš”)
        cmi_data_path = "./data/cmi_data.csv"  # ì‹¤ì œ CMI ë°ì´í„° ê²½ë¡œ
        voice_dataset_path = "./data/voice_dataset"  # ì‹¤ì œ ìŒì„± ë°ì´í„°ì…‹ ê²½ë¡œ
        output_base_path = "./output/fusion_training"
        
        # í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        success = trainer.run_complete_training_pipeline(
            cmi_data_path=cmi_data_path,
            voice_dataset_path=voice_dataset_path,
            output_base_path=output_base_path
        )
        
        if success:
            logger.info("ğŸ‰ ìœµí•© ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
            
            # í›ˆë ¨ ìš”ì•½ ì¶œë ¥
            summary = trainer.get_training_summary()
            logger.info(f"í›ˆë ¨ ìš”ì•½: {summary}")
        else:
            logger.error("âŒ ìœµí•© ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨")
        
    except Exception as e:
        logger.error(f"ë©”ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    main()
